{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b70b34e",
   "metadata": {},
   "source": [
    "### Basic library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719d15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8911e33",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3136aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = '../dataset/'\n",
    "train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "sample_test = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test.csv'))\n",
    "sample_test_out = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test_out.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea6a4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"C://Users//ASUS//Desktop//student_resource 3//to_merge\"\n",
    "file_path = None\n",
    "df_m = pd.read_csv(\"C://Users//ASUS//Desktop//student_resource 3//to_merge//final_2.csv\")\n",
    "for files in os.listdir(root)[1:]:\n",
    "    file_path = os.path.join(root,files)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_m = pd.concat([df_m,df],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fd71f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df_m.sort_values(['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c6e55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df_m.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f5d9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.to_csv(\"merge_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c7a07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean = pd.read_csv(\"finall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24e4d4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(\"sanjith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4daff466",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(to_clean)):\n",
    "    record = to_clean.loc[i]\n",
    "    try:\n",
    "        if \"volt\" in record['prediction']:\n",
    "            to_clean.loc[i,'prediction'] = record['prediction'].replace(\"volt\",\"voltage\")\n",
    "        if int(record['prediction']) or float(record['prediction']):\n",
    "            to_clean.loc[i,'prediction'] = \"\"\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eea9a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean = to_clean.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "873b6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean.to_csv(\"finall1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e260d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean = pd.read_csv(\"merge_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a24e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"C://Users//ASUS//Desktop//student_resource 3//submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34a4dfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         index        prediction\n",
      "0            0   6.68 centimetre\n",
      "1            1   42.0 centimetre\n",
      "2            2  200.0 centimetre\n",
      "3            3                  \n",
      "4            4               NaN\n",
      "...        ...               ...\n",
      "131182  131283       500.0 pound\n",
      "131183  131284                  \n",
      "131184  131285                  \n",
      "131185  131286                  \n",
      "131186  131287            955123\n",
      "\n",
      "[131187 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result = pd.merge(submission, to_clean, on='index', how='left', suffixes=('', '_df2'))\n",
    "\n",
    "# Fill in the empty predictions from df2 or keep the empty string where no match\n",
    "result['prediction'] = result['prediction_df2'].fillna(result['prediction'])\n",
    "\n",
    "# Drop the extra column from df2\n",
    "result = result.drop(columns=['prediction_df2'])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90f1c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea7acd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"final.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebd689",
   "metadata": {},
   "source": [
    "### Run Sanity check using src/sanity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81bb3988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Invalid format in 27.6\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ../dataset/test.csv --output_filename ../finall.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa79459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Invalid unit [lbs] found in 6.75 lbs. Allowed units: {'milligram', 'gallon', 'microgram', 'volt', 'inch', 'fluid ounce', 'kilovolt', 'gram', 'kilogram', 'metre', 'cubic inch', 'watt', 'foot', 'centimetre', 'pound', 'pint', 'yard', 'centilitre', 'litre', 'decilitre', 'millilitre', 'microlitre', 'ounce', 'imperial gallon', 'cup', 'quart', 'ton', 'millivolt', 'millimetre', 'cubic foot', 'kilowatt'}\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ../dataset/sample_test.csv --output_filename ../dataset/sample_test_out_fail.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe930a8",
   "metadata": {},
   "source": [
    "### Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import download_images\n",
    "download_images(train['image_link'], '../images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89aaba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(os.listdir('../images')) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c51918",
   "metadata": {},
   "source": [
    "**PreProcess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6da1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"C://Users//ASUS//Desktop//student_resource 3//dataset//train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e342e4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test['group_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112439fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"C://Users//ASUS//Desktop//student_resource 3//dataset//test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "deafcffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train['group_id'].unique().tolist()).difference(set(df_test['group_id'].unique().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be68735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_check_test_groups(df,filtered_groups, column):\n",
    "    filtered_grp = list(df[column].unique())\n",
    "    filtered_grp, filtered_groups = set(filtered_grp), set(filtered_groups)\n",
    "    filtered_groups_difference = filtered_groups.difference(filtered_grp)\n",
    "    return filtered_groups_difference, len(filtered_groups_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64a9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_grp(df,df_test,column):\n",
    "    grp = df.groupby(column)\n",
    "    filtered_grp = [key for key, group in grp if len(group) >= 100]\n",
    "    diff, len_of_diff = cross_check_test_groups(df_test,filtered_grp,'group_id')\n",
    "    if len_of_diff > 0:\n",
    "        filtered_grp.extend(list(diff))\n",
    "    filtered_df = df[df[column].isin(filtered_grp)]\n",
    "    return filtered_df, filtered_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c368d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_without_images(df : pd.DataFrame):\n",
    "    to_be_included = []\n",
    "    for i in range(len(df)):\n",
    "        img_path = df.loc[i]['image_link'].split(\"/\")[-1]\n",
    "        img_path = os.path.join(\"C://Users//ASUS//Desktop//student_resource 3//images\",img_path)\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        to_be_included.append(df.loc[i]['image_link'])\n",
    "    out = df[df['image_link'].isin(to_be_included)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d283777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter_data_without_images(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1145fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, filtered_groups = filter_by_grp(data,df_test,'group_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97ad44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff, len_of_diff = cross_check_test_groups(df_test,filtered_groups,'group_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ddd66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_of_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be06277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab760d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_set = {\n",
    "    'cm', 'ft', 'in', 'm', 'mm', 'yd',\n",
    "    'g', 'gm', 'gms', 'kg', 'µg', 'mcg', 'mg', 'oz', 'ounce', 'lb', 'lbs', 'ton', 'tons',\n",
    "    'kv', 'mv', 'v', 'volt', \n",
    "    'kw', 'w', 'watt',\n",
    "    'cl', 'centilitre', 'cu ft', 'cubic foot', 'cu in', 'cubic inch', 'cup', 'dl', 'decilitre', \n",
    "    'fl oz', 'fluid ounce', 'gal', 'gallon', 'imp gal', 'imperial gallon', 'l', 'litre', 'µl', 'microlitre', \n",
    "    'ml', 'millilitre', 'pt', 'pint', 'qt', 'quart'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b809c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_value(match):\n",
    "    number = re.sub(r'[^\\d.]', '', match.group(1))\n",
    "    if number.count('.') > 1:\n",
    "        parts = number.split('.')\n",
    "        number = f\"{parts[0]}.{parts[1]}\"\n",
    "    \n",
    "\n",
    "    unit = match.group(2).strip()\n",
    "    return f\"{number} {unit}\"\n",
    "\n",
    "pattern = re.compile(r'([0-9a-zA-Z.]+)\\s*([a-zA-Z]+)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cdce422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14.54 kv', '54.564 mm', '3 mm']\n"
     ]
    }
   ],
   "source": [
    "unit_pattern = r'\\b(?:' + '|'.join(re.escape(unit) for unit in unit_set) + r')\\b'\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'(\\d+\\.?\\d*)\\s*(' + unit_pattern + r')')\n",
    "\n",
    "def extract_units(text):\n",
    "    matches = pattern.findall(text)\n",
    "    results = [f\"{match[0]} {match[1]}\" for match in matches]\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "text = \"weight = 17lbs, 14.54 kv, 54.564   mm, 23k.3s3 mm\"\n",
    "extracted_units = extract_units(text)\n",
    "print(extracted_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbdc53ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' t', '17 s', '14.54 kv', '54.564 mm', '23.33 mm']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"weight = 17lbs, 14.54 kv, 54.564   mm, 23k.3s3 mm\"\n",
    "\n",
    "# Apply the regex and clean each match\n",
    "cleaned_results = [clean_extracted_value(match) for match in pattern.finditer(text)]\n",
    "\n",
    "print(cleaned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5e2ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [i for i in pattern.finditer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f3cff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = re.sub(r'[^\\d.]', '', res[0].group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20c2874c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"17 lbs\", \"14.54 kv\", \"54.564 mm\", \"23.33 mm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3cd52",
   "metadata": {},
   "source": [
    "**transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7a8260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertModel, \n",
    "    RobertaTokenizer, RobertaModel, \n",
    "    AutoTokenizer, AutoModel,\n",
    "    TrOCRProcessor, VisionEncoderDecoderModel, DonutProcessor\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from PIL import Image, ImageOps\n",
    "from io import BytesIO\n",
    "from paddleocr import PaddleOCR\n",
    "import easyocr\n",
    "import re\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C://Users//ASUS//Desktop//student_resource 3//dataset//train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0708a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_without_images(df : pd.DataFrame):\n",
    "    to_be_included = []\n",
    "    for i in range(len(df)):\n",
    "        img_path = df.loc[i]['image_link'].split(\"/\")[-1]\n",
    "        img_path = os.path.join(\"C://Users//ASUS//Desktop//student_resource 3//images\",img_path)\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        to_be_included.append(df.loc[i]['image_link'])\n",
    "    out = df[df['image_link'].isin(to_be_included)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b669dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_for_hdl(df):\n",
    "    filtered_df = df[df['entity_name'].isin(['depth', 'height', 'width'])]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9aad200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedColumns:\n",
    "\n",
    "    def __init__(self, use_pca=False, pca_components=200):\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.pca = PCA(n_components=pca_components) if use_pca else None\n",
    "        self.use_pca = use_pca\n",
    "\n",
    "    def __set_model(self, kind):\n",
    "        if kind == \"DistilBERT\":\n",
    "            self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "            self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        elif kind == \"RoBERTa\":\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "            self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        elif kind == \"MiniLM\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            self.model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        else:\n",
    "            raise ValueError(f\"Model kind '{kind}' not supported\")\n",
    "\n",
    "    def __get_pca(self, x):\n",
    "        if self.pca is not None:\n",
    "            return self.pca.transform(x.reshape(1, -1))\n",
    "        return x\n",
    "\n",
    "    def __apply_pooling(self, out, strategy='mean'):\n",
    "        \"\"\"\n",
    "        Applies pooling strategies to get a fixed-size embedding.\n",
    "        'mean' or 'max' pooling supported.\n",
    "        \"\"\"\n",
    "        if strategy == 'mean':\n",
    "            return out.mean(dim=1)\n",
    "        elif strategy == 'max':\n",
    "            return out.max(dim=1).values\n",
    "        elif strategy == 'cls':\n",
    "            return out[:, 0, :]\n",
    "        else:\n",
    "            raise ValueError(f\"Pooling strategy '{strategy}' not supported\")\n",
    "\n",
    "    def fit_pca(self, dataset_embeddings):\n",
    "        \"\"\"\n",
    "        Fit PCA to a larger dataset of embeddings to avoid fitting on single inputs.\n",
    "        \"\"\"\n",
    "        if self.pca:\n",
    "            self.pca.fit(dataset_embeddings)\n",
    "\n",
    "    def get_embeddings(self, val, kind=\"DistilBERT\", pooling_strategy='cls'):\n",
    "        \"\"\"\n",
    "        Get embeddings for the input text. Supports optional pooling strategies.\n",
    "        \"\"\"\n",
    "        self.__set_model(kind)\n",
    "        inputs = self.tokenizer(val, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        pooled_output = self.__apply_pooling(hidden_states, strategy=pooling_strategy)\n",
    "\n",
    "        embedding = pooled_output.squeeze(0).numpy()\n",
    "\n",
    "        if self.use_pca:\n",
    "            embedding = self.__get_pca(embedding)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9fff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRFeatureExtractor:\n",
    "    def __init__(self, model_name=\"TrOCR\", pca_components=200):\n",
    "        self.pca_components = pca_components\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        if model_name == \"TrOCR\":\n",
    "            self.processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "            self.model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "        elif model_name == \"Donut\":\n",
    "            self.processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "            self.model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model name. Use 'TrOCR' or 'Donut'.\")\n",
    "    def get_arch(self):\n",
    "        return self.model\n",
    "    \n",
    "    def extract_features(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.model.encoder(pixel_values).last_hidden_state\n",
    "        \n",
    "        encoder_features = encoder_outputs.squeeze(0).numpy()\n",
    "        \n",
    "        return encoder_features\n",
    "\n",
    "    def extract_features_from_folder(self, folder_path):\n",
    "        import os\n",
    "        features_list = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(image_path):\n",
    "                features = self.extract_features(image_path)\n",
    "                features_list.append(features)\n",
    "        \n",
    "        return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3841ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, use_resnet=True):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.use_resnet = use_resnet\n",
    "        \n",
    "        if use_resnet:\n",
    "            # Initialize ResNet50 and move it to GPU if available\n",
    "            self.resnet = models.resnet50(pretrained=True).to(self.device).eval()\n",
    "            \n",
    "            # Preprocessing pipeline for ResNet\n",
    "            self.preprocess_resnet = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),  # ResNet50 input size\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    def extract_features(self, image_path):\n",
    "        \"\"\"Extract image features using ResNet and run the model on CUDA if available.\"\"\"\n",
    "        # Load and preprocess the image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = self.preprocess_resnet(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Move the image tensor to GPU (if available)\n",
    "        img_tensor = img_tensor.to(self.device)\n",
    "\n",
    "        # Extract features with ResNet\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(img_tensor)\n",
    "\n",
    "        # Move features back to CPU and convert to NumPy array\n",
    "        return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4613ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "\n",
    "    def __init__(self, data, ocr_model = \"Donut\",output_file = \"output.csv\",batch_size = 500,\n",
    "                 use_group_id  = False,\n",
    "                 use_resnet = False,\n",
    "                 image_root_path = \"\",\n",
    "                 max_metric_count = 5,\n",
    "                 kind = \"train\"):\n",
    "\n",
    "        self.data = data\n",
    "        self.output_file = output_file\n",
    "        self.batch_size = batch_size\n",
    "        unique_grp_ids = sorted(list(data[\"group_id\"].unique()))\n",
    "\n",
    "        self.rev_group_id_mappings = {i : grp_id for i, grp_id in enumerate(unique_grp_ids)}\n",
    "        self.group_id_mappings = {grp_id : i for i, grp_id in enumerate(unique_grp_ids)}\n",
    "        self.ocr_feature_extrac = OCRFeatureExtractor(ocr_model)\n",
    "        self.max_metric_count = max_metric_count\n",
    "        if use_resnet:\n",
    "            self.resnet_feature_extractor = ImageFeatureExtractor()\n",
    "        self.image_root_path = image_root_path\n",
    "        self.kind = kind\n",
    "        self.s_one_hot_encode = OneHotEncoder()\n",
    "        self.met_one_hot = OneHotEncoder()\n",
    "        self.ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "        self.use_resnet = use_resnet\n",
    "        self.use_group_id = use_group_id\n",
    "        self.map_to_unit= {\n",
    "            # Length\n",
    "            'cm': 'centimetre', 'mm': 'millimetre', 'm': 'metre', 'in': 'inch', 'ft': 'foot', 'yd': 'yard',\n",
    "            \n",
    "            # Weight\n",
    "            'g': 'gram', 'gm': 'gram', 'gms': 'gram', 'kg': 'kilogram', 'mg': 'milligram', 'µg': 'microgram',\n",
    "            'mcg': 'microgram', 'oz': 'ounce', 'lb': 'pound', 'lbs': 'pound', 'ton': 'ton', 'tons': 'ton',\n",
    "            \n",
    "            # Voltage\n",
    "            'v': 'volt', 'kv': 'kilovolt', 'mv': 'millivolt',\n",
    "            \n",
    "            # Wattage\n",
    "            'w': 'watt', 'kw': 'kilowatt',\n",
    "            \n",
    "            # Volume\n",
    "            'cl': 'centilitre', 'cu ft': 'cubic foot', 'cubic foot': 'cubic foot', 'cu in': 'cubic inch', \n",
    "            'cubic inch': 'cubic inch', 'cup': 'cup', 'dl': 'decilitre', 'decilitre': 'decilitre', 'fl oz': 'fluid ounce', \n",
    "            'fluid ounce': 'fluid ounce', 'gal': 'gallon', 'gallon': 'gallon', 'imp gal': 'imperial gallon',\n",
    "            'imperial gallon': 'imperial gallon', 'l': 'litre', 'litre': 'litre', 'µl': 'microlitre', 'microlitre': 'microlitre',\n",
    "            'ml': 'millilitre', 'millilitre': 'millilitre', 'pt': 'pint', 'pint': 'pint', 'qt': 'quart', 'quart': 'quart'\n",
    "        }\n",
    "        self.unique_metrics = list(set(self.map_to_unit.values()))\n",
    "        self.unique_metrics.append(\"NAN\")\n",
    "        self.__fit_one_hot()\n",
    "        \n",
    "# Example usage\n",
    "    def __get_full_unit_name(self,unit):\n",
    "        if unit in self.unique_metrics:\n",
    "            return unit\n",
    "        out = self.map_to_unit.get(unit.lower(), unit)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def get_one_hot_encode(self):\n",
    "        return self.s_one_hot_encode\n",
    "\n",
    "    def __fit_one_hot(self):\n",
    "        self.s_one_hot_encode.fit(self.data[['entity_name']])\n",
    "        self.met_one_hot.fit([[i] for i in self.unique_metrics])\n",
    "\n",
    "    def extract_value_unit_from_image(self,image_path):\n",
    "            img = Image.open(image_path)\n",
    "            gray_img = ImageOps.grayscale(img)\n",
    "            \n",
    "            np_image = np.array(gray_img)\n",
    "            result = self.ocr.ocr(np_image, cls=True)\n",
    "            extracted_text = \" \".join([res[1][0] for res in result[0]])\n",
    "            pattern = r'(\\d+\\.?\\d*)\\s?(cm³|liters|ml|g|kg|m³|in³|L|oz|fl oz|lb|centimetre|gram|l|pt|metre|g|cm|ton|ft|volt|millilitre|millimetre|kg|v|millivolt|imperial gallon|centilitre|cl|gal|m|kv|microlitre|qt|mv|microgram|w|milligram|µl|lbs|imp gal|fluid ounce|litre|tons|gallon|pound|quart|µg|foot|mg|ounce|mcg|kilovolt|cubic foot|gm|kilowatt|yd|cup|dl|oz|mm|cu ft|kw|cubic inch|gms|yard|kilogram|in|watt|fl oz|inch|decilitre|ml|pint|cu in)'  # Modify as per expected units\n",
    "            matches = re.findall(pattern, extracted_text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                return matches\n",
    "            else:\n",
    "                return \"\"\n",
    "    def get_group_id_mappings(self):\n",
    "        return self.group_id_mappings, self.rev_group_id_mappings\n",
    "\n",
    "    def __transform_for_one_record(self,record):\n",
    "        image_link ,group_id, entity_name, entity_value = None, None, None, None\n",
    "        if self.kind == \"train\":\n",
    "            image_link ,group_id, entity_name, entity_value = record['image_link'], record['group_id'], record['entity_name'], record['entity_value']\n",
    "        elif self.kind == \"test\":\n",
    "            image_link ,group_id, entity_name = record['image_link'], record['group_id'], record['entity_name']\n",
    "        try:\n",
    "            img_path = os.path.join(self.image_root_path,image_link.split(\"/\")[-1])\n",
    "            transformed_img = self.ocr_feature_extrac.extract_features(img_path)\n",
    "            resnet_features = self.resnet_feature_extractor.extract_features(img_path) if self.use_resnet else None\n",
    "            transformed_grp = self.group_id_mappings[group_id] if self.use_group_id else None\n",
    "            transformed_entity_name = self.s_one_hot_encode.transform([[entity_name]]).toarray()\n",
    "            if entity_value:\n",
    "                value, unit = entity_value.split(\" \")\n",
    "                value = float(value)\n",
    "                transformed_unit = self.met_one_hot.transform([[unit]])\n",
    "                entity_value = (value, transformed_unit)\n",
    "            \n",
    "            matches = self.extract_value_unit_from_image(img_path)\n",
    "            metric_set = []\n",
    "            if matches:\n",
    "                if len(matches) > self.max_metric_count:\n",
    "                    matches = matches[:self.max_metric_count]\n",
    "                while len(matches) < self.max_metric_count:\n",
    "                    matches.append(('0', 'NAN'))\n",
    "                metric_set = []\n",
    "                for i in range(len(matches)):\n",
    "                    val , uni = matches[i][0], matches[i][1]\n",
    "\n",
    "                    uni = uni.strip()\n",
    "                    mapped_unit = self.__get_full_unit_name(uni)\n",
    "                    \n",
    "\n",
    "                    transformed_unit = self.met_one_hot.transform([[mapped_unit]])\n",
    "\n",
    "                    val = float(val)\n",
    "\n",
    "                    metric_set.append([val,transformed_unit])\n",
    "            else:\n",
    "                with open(\"log.txt\",\"w\") as f:\n",
    "                    f.write(f\"{image_link} \\n\")\n",
    "                return\n",
    "            return resnet_features, transformed_img, transformed_grp, transformed_entity_name, entity_value, metric_set\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            with open(\"logs.txt\",\"w\") as f:\n",
    "                f.write(f\"{image_link} {str(e)} \\n\")\n",
    "            return None\n",
    "    \n",
    "    def preprocess(self):\n",
    "        processed_data = []\n",
    "        for idx, record in tqdm(self.data.iterrows()):\n",
    "            result = self.__transform_for_one_record(record)\n",
    "            if result:\n",
    "                processed_data.append(result)\n",
    "                if (idx + 1) % self.batch_size == 0 or (idx + 1) == len(self.data):\n",
    "                    self.save_to_disk(processed_data)\n",
    "                    processed_data = []\n",
    "\n",
    "    def save_to_disk(self, processed_data):\n",
    "        df = pd.DataFrame(processed_data, columns=['resnet_features', 'group_id', 'entity_name_onehot', 'metric_set'])\n",
    "        if not os.path.exists(self.output_file):\n",
    "            df.to_csv(self.output_file, mode='w', header=True, index=False)\n",
    "        else:\n",
    "            df.to_csv(self.output_file, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96354e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = filter_data_without_images(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3650991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = filter_data_for_hdl(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f272d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132907"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144fdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53977cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 14:47:25] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\ASUS\\\\Desktop\\\\student_resource 3\\\\env\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "preprocess = PreProcessPipeline(train,use_group_id=True,use_resnet=True, image_root_path=\"C://Users//ASUS//Desktop//student_resource 3//images\",max_metric_count=10,batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30d85607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 14:49:23] ppocr DEBUG: dt_boxes num : 17, elapsed : 1.2302513122558594\n",
      "[2024/09/15 14:49:23] ppocr DEBUG: cls num  : 17, elapsed : 0.33968091011047363\n",
      "[2024/09/15 14:49:26] ppocr DEBUG: rec_res num  : 17, elapsed : 2.4046502113342285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:57, 117.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 14:51:18] ppocr DEBUG: dt_boxes num : 15, elapsed : 0.45172929763793945\n",
      "[2024/09/15 14:51:19] ppocr DEBUG: cls num  : 15, elapsed : 0.4559652805328369\n",
      "[2024/09/15 14:51:22] ppocr DEBUG: rec_res num  : 15, elapsed : 3.323594093322754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [03:54, 117.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 14:53:17] ppocr DEBUG: dt_boxes num : 53, elapsed : 0.2146167755126953\n",
      "[2024/09/15 14:53:17] ppocr DEBUG: cls num  : 53, elapsed : 0.20499730110168457\n",
      "[2024/09/15 14:53:33] ppocr DEBUG: rec_res num  : 53, elapsed : 16.39624333381653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [06:05, 123.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 14:55:52] ppocr DEBUG: dt_boxes num : 43, elapsed : 0.21697616577148438\n",
      "[2024/09/15 14:55:52] ppocr DEBUG: cls num  : 43, elapsed : 0.31307435035705566\n",
      "[2024/09/15 14:56:04] ppocr DEBUG: rec_res num  : 43, elapsed : 12.242830514907837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [08:36, 134.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 14:57:38] ppocr DEBUG: dt_boxes num : 13, elapsed : 0.28879261016845703\n",
      "[2024/09/15 14:57:38] ppocr DEBUG: cls num  : 13, elapsed : 0.1240227222442627\n",
      "[2024/09/15 14:57:39] ppocr DEBUG: rec_res num  : 13, elapsed : 0.8581042289733887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [10:11, 120.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 14:59:08] ppocr DEBUG: dt_boxes num : 17, elapsed : 0.2093524932861328\n",
      "[2024/09/15 14:59:08] ppocr DEBUG: cls num  : 17, elapsed : 0.19489026069641113\n",
      "[2024/09/15 14:59:10] ppocr DEBUG: rec_res num  : 17, elapsed : 1.6470816135406494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [11:42, 110.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 15:01:08] ppocr DEBUG: dt_boxes num : 37, elapsed : 0.21521210670471191\n",
      "[2024/09/15 15:01:08] ppocr DEBUG: cls num  : 37, elapsed : 0.1704721450805664\n",
      "[2024/09/15 15:01:11] ppocr DEBUG: rec_res num  : 37, elapsed : 3.069014549255371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [13:43, 113.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 15:02:57] ppocr DEBUG: dt_boxes num : 23, elapsed : 0.584296703338623\n",
      "[2024/09/15 15:02:57] ppocr DEBUG: cls num  : 23, elapsed : 0.2313084602355957\n",
      "[2024/09/15 15:03:00] ppocr DEBUG: rec_res num  : 23, elapsed : 2.7002670764923096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [15:31, 112.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 15:04:55] ppocr DEBUG: dt_boxes num : 24, elapsed : 0.22770071029663086\n",
      "[2024/09/15 15:04:55] ppocr DEBUG: cls num  : 24, elapsed : 0.10706806182861328\n",
      "[2024/09/15 15:04:57] ppocr DEBUG: rec_res num  : 24, elapsed : 1.687105655670166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [17:29, 113.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 15:07:17] ppocr DEBUG: dt_boxes num : 30, elapsed : 0.22754526138305664\n",
      "[2024/09/15 15:07:17] ppocr DEBUG: cls num  : 30, elapsed : 0.14246296882629395\n",
      "[2024/09/15 15:07:24] ppocr DEBUG: rec_res num  : 30, elapsed : 6.5718607902526855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [19:56, 124.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 15:09:55] ppocr DEBUG: dt_boxes num : 7, elapsed : 0.21075773239135742\n",
      "[2024/09/15 15:09:55] ppocr DEBUG: cls num  : 7, elapsed : 0.1681656837463379\n",
      "[2024/09/15 15:10:03] ppocr DEBUG: rec_res num  : 7, elapsed : 7.433466672897339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [25:12, 137.47s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 136\u001b[0m, in \u001b[0;36mPreProcessPipeline.preprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, record \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[1;32m--> 136\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__transform_for_one_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    138\u001b[0m         processed_data\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[7], line 128\u001b[0m, in \u001b[0;36mPreProcessPipeline.__transform_for_one_record\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resnet_features, transformed_img, transformed_grp, transformed_entity_name, entity_value, metric_set\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    130\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 97\u001b[0m, in \u001b[0;36mPreProcessPipeline.__transform_for_one_record\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m     95\u001b[0m transformed_entity_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_one_hot_encode\u001b[38;5;241m.\u001b[39mtransform([[entity_name]])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entity_value:\n\u001b[1;32m---> 97\u001b[0m     value, unit \u001b[38;5;241m=\u001b[39m entity_value\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(value)\n\u001b[0;32m     99\u001b[0m     transformed_unit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmet_one_hot\u001b[38;5;241m.\u001b[39mtransform([[unit]])\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "preprocess.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2016dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/14 23:20:45] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\ASUS\\\\Desktop\\\\student_resource 3\\\\env\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')  # Modify the parameters if necessary\n",
    "\n",
    "def extract_value_unit_from_image(image_path):\n",
    "    # Open and preprocess the image\n",
    "    img = Image.open(image_path)\n",
    "    gray_img = ImageOps.grayscale(img)\n",
    "    \n",
    "    # Convert the image to a numpy array for PaddleOCR\n",
    "    np_image = np.array(gray_img)\n",
    "    \n",
    "    # Perform OCR using PaddleOCR\n",
    "    result = ocr.ocr(np_image, cls=True)\n",
    "    \n",
    "    # Extract the recognized text\n",
    "    extracted_text = \" \".join([res[1][0] for res in result[0]])  # Concatenate the OCR text\n",
    "    \n",
    "    # Use regex to find value + unit (e.g., numbers followed by units like cm³, L, ml, etc.)\n",
    "    pattern = r'(\\d+\\.?\\d*)\\s?(cm³|liters|ml|g|kg|m³|in³|L|oz|fl oz|gallon|µg|kg|ml|litre|fluid ounce|m|ft|quart|kv|fl oz|volt|pt|yd|in|v|tons|gal|l|ounce|cl|µl|lbs|microlitre|mg|w|cubic inch|cm|lb|cubic foot|watt|cu in|cu ft|g|pint|mcg|ton|dl|decilitre|imp gal|cup|gms|gm|centilitre|kw|imperial gallon|mm|millilitre|oz|qt|mv|gallon)'  # Modify as per expected units\n",
    "    matches = re.findall(pattern, extracted_text, re.IGNORECASE)\n",
    "    \n",
    "    if matches:\n",
    "        # Return the first match (value + unit)\n",
    "        return matches\n",
    "    else:\n",
    "        # Return blank if no match is found\n",
    "        return \" \"\n",
    "\n",
    "# Example usage:\n",
    "# result = extract_value_unit_from_image('path_to_image.jpg')\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f547dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/14 23:20:55] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.08312630653381348\n",
      "[2024/09/14 23:20:55] ppocr DEBUG: cls num  : 4, elapsed : 0.026586055755615234\n",
      "[2024/09/14 23:20:55] ppocr DEBUG: rec_res num  : 4, elapsed : 0.11325645446777344\n"
     ]
    }
   ],
   "source": [
    "matches = extract_value_unit_from_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397c64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.985 cm\n",
      "2.75 in\n",
      "182.88 cm\n",
      "381 cm\n",
      "72 in\n",
      "150 in\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(matches)):\n",
    "    result = extract_value_unit_from_image(i)\n",
    "    print(matches[i][0], matches[i][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b6a12ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reader = easyocr.Reader(['en'], gpu=True)  # Set gpu=True for GPU usage\n",
    "\n",
    "# Function to preprocess and perform OCR on the image URL\n",
    "def extract_value_unit_from_image(image_url):\n",
    "    try:\n",
    "        # Send a request to get the image from the URL\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "        # Open the image and convert it to grayscale\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        gray_img = ImageOps.grayscale(img)\n",
    "        \n",
    "        # Convert Pillow image to numpy array for OCR\n",
    "        np_image = np.array(gray_img)\n",
    "        \n",
    "        # Perform OCR using EasyOCR\n",
    "        result = reader.readtext(np_image, detail=0)  # detail=0 returns only the text\n",
    "        extracted_text = \" \".join(result)  # Join the OCR result into a single string\n",
    "        \n",
    "        # Use regex to find the value + unit (e.g., numbers followed by units like cm³, L, ml, etc.)\n",
    "        pattern = r'(\\d+\\.?\\d*)\\s?(cm³|liters|ml|g|kg|m³|in³|L|oz|fl oz|gallon|µg|kg|ml|litre|fluid ounce|m|ft|quart|kv|fl oz|volt|pt|yd|in|v|tons|gal|l|ounce|cl|µl|lbs|microlitre|mg|w|cubic inch|cm|lb|cubic foot|watt|cu in|cu ft|g|pint|mcg|ton|dl|decilitre|imp gal|cup|gms|gm|centilitre|kw|imperial gallon|mm|millilitre|oz|qt|mv)'  # Modify as per expected units\n",
    "        matches = re.findall(pattern, extracted_text, re.IGNORECASE)\n",
    "        \n",
    "        if matches:\n",
    "            # Return the first match (value + unit)\n",
    "            return f\"{matches[0][0]} {matches[0][1]}\"\n",
    "        else:\n",
    "            # Return blank if no match is found\n",
    "            return \" \"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image from {image_url}: {e}\")\n",
    "        return \" \"  # Return blank in case of an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f29b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_set = {'cm', 'ft', 'in', 'm', 'mm', 'yd','g', 'gm', 'gms', 'kg', 'µg', 'mcg', 'mg', 'oz', 'ounce', 'lb', 'lbs', 'ton', 'tons','kv', 'mv', 'v', 'volt', 'kw', 'w', 'watt','cl', 'centilitre', 'cu ft', 'cubic foot', 'cu in', 'cubic inch', 'cup', 'dl', 'decilitre', 'fl oz', 'fluid ounce', 'gal', 'gallon', 'imp gal', 'imperial gallon', 'l', 'litre', 'µl', 'microlitre', 'ml', 'millilitre', 'pt', 'pint', 'qt', 'quart', 'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard', 'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton', 'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton', 'kilovolt', 'millivolt', 'volt' , 'kilowatt', 'watt', 'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59ae73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43843b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = '|'.join(unit for unit in unit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a0ce88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lb|centimetre|gram|l|pt|metre|g|cm|ton|ft|volt|millilitre|millimetre|kg|v|millivolt|imperial gallon|centilitre|cl|gal|m|kv|microlitre|qt|mv|microgram|w|milligram|µl|lbs|imp gal|fluid ounce|litre|tons|gallon|pound|quart|µg|foot|mg|ounce|mcg|kilovolt|cubic foot|gm|kilowatt|yd|cup|dl|oz|mm|cu ft|kw|cubic inch|gms|yard|kilogram|in|watt|fl oz|inch|decilitre|ml|pint|cu in'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33621d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_weight', 'item_volume', 'voltage', 'wattage',\n",
       "       'maximum_weight_recommendation', 'height', 'depth', 'width'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['entity_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22929cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_value_unit_from_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b794f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051818f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.fit(train[['entity_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12f07d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.transform([[\"item_volume\"]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "500e080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out = embed.get_embeddings(\"voltage\",\"DistilBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4866097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_link      https://m.media-amazon.com/images/I/61I9XdN6OF...\n",
       "group_id                                                   748919\n",
       "entity_name                                           item_weight\n",
       "entity_value                                           500.0 gram\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8654cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "facc28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--naver-clova-ix--donut-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "mod = OCRFeatureExtractor(model_name=\"Donut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b1a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mod.extract_features(\"C://Users//ASUS//Desktop//student_resource 3//images//31+ZLMVIYaL.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a144599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 1024)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f45948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "028a83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pca.fit_transform(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f0c044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9deacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17dd8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nn.Linear(200, 200)\n",
    "n2 = nn.Linear(200,50)\n",
    "n3 = nn.Linear(240000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb2bfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = n2(n(torch.tensor(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5865bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.view(out.shape[0]*out.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06b0fa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "260f4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = n3(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f21094a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e4fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:49<00:00, 24.76s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Loading C://Users//ASUS//Desktop//student_resource 3//MiniCPM-V-2 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC://Users//ASUS//Desktop//student_resource 3//MiniCPM-V-2\u001b[39m\u001b[38;5;124m'\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# sdpa or flash_attention_2, no eager\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m----> 9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC://Users//ASUS//Desktop//student_resource 3//MiniCPM-V-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:869\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m tokenizer_auto_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    862\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    863\u001b[0m     config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m     )\n\u001b[0;32m    868\u001b[0m )\n\u001b[1;32m--> 869\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_fast \u001b[38;5;129;01mand\u001b[39;00m tokenizer_auto_map[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:640\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[1;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[0;32m    637\u001b[0m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires you to execute the configuration file in that\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    644\u001b[0m     )\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[1;31mValueError\u001b[0m: Loading C://Users//ASUS//Desktop//student_resource 3//MiniCPM-V-2 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = AutoModel.from_pretrained('C://Users//ASUS//Desktop//student_resource 3//MiniCPM-V-2', trust_remote_code=True) # sdpa or flash_attention_2, no eager\n",
    "model = model.eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedcfe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('C://Users//ASUS//Desktop//student_resource 3//MiniCPM-V-2', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56447815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy:\n",
    "    def __init__(self,a):\n",
    "        self.a = a\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.a}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1110ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = dummy(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892daa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac691ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
