{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b70b34e",
   "metadata": {},
   "source": [
    "### Basic library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719d15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8911e33",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3136aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = '../dataset/'\n",
    "train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "sample_test = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test.csv'))\n",
    "sample_test_out = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test_out.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebd689",
   "metadata": {},
   "source": [
    "### Run Sanity check using src/sanity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bb3988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing successfull for file: ../dataset/sample_test_out.csv\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ../dataset/sample_test.csv --output_filename ../dataset/sample_test_out.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa79459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Invalid unit [lbs] found in 6.75 lbs. Allowed units: {'milligram', 'gallon', 'microgram', 'volt', 'inch', 'fluid ounce', 'kilovolt', 'gram', 'kilogram', 'metre', 'cubic inch', 'watt', 'foot', 'centimetre', 'pound', 'pint', 'yard', 'centilitre', 'litre', 'decilitre', 'millilitre', 'microlitre', 'ounce', 'imperial gallon', 'cup', 'quart', 'ton', 'millivolt', 'millimetre', 'cubic foot', 'kilowatt'}\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ../dataset/sample_test.csv --output_filename ../dataset/sample_test_out_fail.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe930a8",
   "metadata": {},
   "source": [
    "### Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import download_images\n",
    "download_images(train['image_link'], '../images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89aaba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(os.listdir('../images')) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c51918",
   "metadata": {},
   "source": [
    "**PreProcess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6da1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"C://Users//ASUS//Desktop//student_resource 3//dataset//train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e342e4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test['group_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112439fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"C://Users//ASUS//Desktop//student_resource 3//dataset//test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "deafcffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train['group_id'].unique().tolist()).difference(set(df_test['group_id'].unique().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be68735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_check_test_groups(df,filtered_groups, column):\n",
    "    filtered_grp = list(df[column].unique())\n",
    "    filtered_grp, filtered_groups = set(filtered_grp), set(filtered_groups)\n",
    "    filtered_groups_difference = filtered_groups.difference(filtered_grp)\n",
    "    return filtered_groups_difference, len(filtered_groups_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64a9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_grp(df,df_test,column):\n",
    "    grp = df.groupby(column)\n",
    "    filtered_grp = [key for key, group in grp if len(group) >= 100]\n",
    "    diff, len_of_diff = cross_check_test_groups(df_test,filtered_grp,'group_id')\n",
    "    if len_of_diff > 0:\n",
    "        filtered_grp.extend(list(diff))\n",
    "    filtered_df = df[df[column].isin(filtered_grp)]\n",
    "    return filtered_df, filtered_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c368d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_without_images(df : pd.DataFrame):\n",
    "    to_be_included = []\n",
    "    for i in range(len(df)):\n",
    "        img_path = df.loc[i]['image_link'].split(\"/\")[-1]\n",
    "        img_path = os.path.join(\"C://Users//ASUS//Desktop//student_resource 3//images\",img_path)\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        to_be_included.append(df.loc[i]['image_link'])\n",
    "    out = df[df['image_link'].isin(to_be_included)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d283777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter_data_without_images(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1145fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, filtered_groups = filter_by_grp(data,df_test,'group_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97ad44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff, len_of_diff = cross_check_test_groups(df_test,filtered_groups,'group_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ddd66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_of_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be06277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab760d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_set = {\n",
    "    'cm', 'ft', 'in', 'm', 'mm', 'yd',\n",
    "    'g', 'gm', 'gms', 'kg', 'µg', 'mcg', 'mg', 'oz', 'ounce', 'lb', 'lbs', 'ton', 'tons',\n",
    "    'kv', 'mv', 'v', 'volt', \n",
    "    'kw', 'w', 'watt',\n",
    "    'cl', 'centilitre', 'cu ft', 'cubic foot', 'cu in', 'cubic inch', 'cup', 'dl', 'decilitre', \n",
    "    'fl oz', 'fluid ounce', 'gal', 'gallon', 'imp gal', 'imperial gallon', 'l', 'litre', 'µl', 'microlitre', \n",
    "    'ml', 'millilitre', 'pt', 'pint', 'qt', 'quart'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b809c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_value(match):\n",
    "    number = re.sub(r'[^\\d.]', '', match.group(1))\n",
    "    if number.count('.') > 1:\n",
    "        parts = number.split('.')\n",
    "        number = f\"{parts[0]}.{parts[1]}\"\n",
    "    \n",
    "\n",
    "    unit = match.group(2).strip()\n",
    "    return f\"{number} {unit}\"\n",
    "\n",
    "pattern = re.compile(r'([0-9a-zA-Z.]+)\\s*([a-zA-Z]+)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cdce422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14.54 kv', '54.564 mm', '3 mm']\n"
     ]
    }
   ],
   "source": [
    "unit_pattern = r'\\b(?:' + '|'.join(re.escape(unit) for unit in unit_set) + r')\\b'\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern = re.compile(r'(\\d+\\.?\\d*)\\s*(' + unit_pattern + r')')\n",
    "\n",
    "def extract_units(text):\n",
    "    matches = pattern.findall(text)\n",
    "    results = [f\"{match[0]} {match[1]}\" for match in matches]\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "text = \"weight = 17lbs, 14.54 kv, 54.564   mm, 23k.3s3 mm\"\n",
    "extracted_units = extract_units(text)\n",
    "print(extracted_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbdc53ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' t', '17 s', '14.54 kv', '54.564 mm', '23.33 mm']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"weight = 17lbs, 14.54 kv, 54.564   mm, 23k.3s3 mm\"\n",
    "\n",
    "# Apply the regex and clean each match\n",
    "cleaned_results = [clean_extracted_value(match) for match in pattern.finditer(text)]\n",
    "\n",
    "print(cleaned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5e2ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [i for i in pattern.finditer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f3cff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = re.sub(r'[^\\d.]', '', res[0].group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20c2874c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"17 lbs\", \"14.54 kv\", \"54.564 mm\", \"23.33 mm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3cd52",
   "metadata": {},
   "source": [
    "**transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7a8260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertModel, \n",
    "    RobertaTokenizer, RobertaModel, \n",
    "    AutoTokenizer, AutoModel,\n",
    "    TrOCRProcessor, VisionEncoderDecoderModel, DonutProcessor\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from PIL import Image, ImageOps\n",
    "from io import BytesIO\n",
    "from paddleocr import PaddleOCR\n",
    "import easyocr\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C://Users//ASUS//Desktop//student_resource 3//dataset//train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42f6493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['group_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fb52833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df_test['group_id'].unique().tolist()).difference(set(df_train['group_id'].unique().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87447af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_test['group_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30d861e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "data = encoder.fit_transform(df_train[['group_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81a583f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de15cc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_weight', 'item_volume', 'voltage', 'wattage',\n",
       "       'maximum_weight_recommendation', 'height', 'depth', 'width'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['entity_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9aad200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedColumns:\n",
    "\n",
    "    def __init__(self, use_pca=False, pca_components=200):\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.pca = PCA(n_components=pca_components) if use_pca else None\n",
    "        self.use_pca = use_pca\n",
    "\n",
    "    def __set_model(self, kind):\n",
    "        if kind == \"DistilBERT\":\n",
    "            self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "            self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        elif kind == \"RoBERTa\":\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "            self.model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        elif kind == \"MiniLM\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            self.model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        else:\n",
    "            raise ValueError(f\"Model kind '{kind}' not supported\")\n",
    "\n",
    "    def __get_pca(self, x):\n",
    "        if self.pca is not None:\n",
    "            return self.pca.transform(x.reshape(1, -1))\n",
    "        return x\n",
    "\n",
    "    def __apply_pooling(self, out, strategy='mean'):\n",
    "        \"\"\"\n",
    "        Applies pooling strategies to get a fixed-size embedding.\n",
    "        'mean' or 'max' pooling supported.\n",
    "        \"\"\"\n",
    "        if strategy == 'mean':\n",
    "            return out.mean(dim=1)\n",
    "        elif strategy == 'max':\n",
    "            return out.max(dim=1).values\n",
    "        elif strategy == 'cls':\n",
    "            return out[:, 0, :]\n",
    "        else:\n",
    "            raise ValueError(f\"Pooling strategy '{strategy}' not supported\")\n",
    "\n",
    "    def fit_pca(self, dataset_embeddings):\n",
    "        \"\"\"\n",
    "        Fit PCA to a larger dataset of embeddings to avoid fitting on single inputs.\n",
    "        \"\"\"\n",
    "        if self.pca:\n",
    "            self.pca.fit(dataset_embeddings)\n",
    "\n",
    "    def get_embeddings(self, val, kind=\"DistilBERT\", pooling_strategy='cls'):\n",
    "        \"\"\"\n",
    "        Get embeddings for the input text. Supports optional pooling strategies.\n",
    "        \"\"\"\n",
    "        self.__set_model(kind)\n",
    "        inputs = self.tokenizer(val, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        pooled_output = self.__apply_pooling(hidden_states, strategy=pooling_strategy)\n",
    "\n",
    "        embedding = pooled_output.squeeze(0).numpy()\n",
    "\n",
    "        if self.use_pca:\n",
    "            embedding = self.__get_pca(embedding)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRFeatureExtractor:\n",
    "    def __init__(self, model_name=\"TrOCR\", pca_components=200):\n",
    "        self.pca_components = pca_components\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        if model_name == \"TrOCR\":\n",
    "            self.processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "            self.model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "        elif model_name == \"Donut\":\n",
    "            self.processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "            self.model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model name. Use 'TrOCR' or 'Donut'.\")\n",
    "    def get_arch(self):\n",
    "        return self.model\n",
    "    \n",
    "    def extract_features(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.model.encoder(pixel_values).last_hidden_state\n",
    "        \n",
    "        encoder_features = encoder_outputs.squeeze(0).numpy()\n",
    "        \n",
    "        return encoder_features\n",
    "\n",
    "    def extract_features_from_folder(self, folder_path):\n",
    "        import os\n",
    "        features_list = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(image_path):\n",
    "                features = self.extract_features(image_path)\n",
    "                features_list.append(features)\n",
    "        \n",
    "        return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4613ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "\n",
    "    def __init__(self, data, ocr_model = \"\", resnet_model = \"\",\n",
    "                 use_group_id  = False,\n",
    "                 use_resnet = False,\n",
    "                 image_root_path = \"\",\n",
    "                 max_metric_count = 5,\n",
    "                 kind = \"train\"):\n",
    "\n",
    "        self.data = data\n",
    "        unique_grp_ids = sorted(list(data[\"group_id\"].unique()))\n",
    "        self.group_id_mappings = {i : grp_id for i, grp_id in enumerate(unique_grp_ids)}\n",
    "        self.rev_group_id_mappings = {grp_id : i for i, grp_id in enumerate(unique_grp_ids)}\n",
    "        self.ocr_feature_extrac = OCRFeatureExtractor(ocr_model)\n",
    "        self.max_metric_count = max_metric_count\n",
    "        if use_resnet:\n",
    "            self.resnet_feature_extractor = None\n",
    "        self.image_root_path = image_root_path\n",
    "        self.kind = kind\n",
    "        self.s_one_hot_encode = OneHotEncoder()\n",
    "        self.met_one_hot = OneHotEncoder()\n",
    "        self.ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "        self.map_to_unit= {\n",
    "            # Length\n",
    "            'cm': 'centimetre', 'mm': 'millimetre', 'm': 'metre', 'in': 'inch', 'ft': 'foot', 'yd': 'yard',\n",
    "            \n",
    "            # Weight\n",
    "            'g': 'gram', 'gm': 'gram', 'gms': 'gram', 'kg': 'kilogram', 'mg': 'milligram', 'µg': 'microgram',\n",
    "            'mcg': 'microgram', 'oz': 'ounce', 'lb': 'pound', 'lbs': 'pound', 'ton': 'ton', 'tons': 'ton',\n",
    "            \n",
    "            # Voltage\n",
    "            'v': 'volt', 'kv': 'kilovolt', 'mv': 'millivolt',\n",
    "            \n",
    "            # Wattage\n",
    "            'w': 'watt', 'kw': 'kilowatt',\n",
    "            \n",
    "            # Volume\n",
    "            'cl': 'centilitre', 'cu ft': 'cubic foot', 'cubic foot': 'cubic foot', 'cu in': 'cubic inch', \n",
    "            'cubic inch': 'cubic inch', 'cup': 'cup', 'dl': 'decilitre', 'decilitre': 'decilitre', 'fl oz': 'fluid ounce', \n",
    "            'fluid ounce': 'fluid ounce', 'gal': 'gallon', 'gallon': 'gallon', 'imp gal': 'imperial gallon',\n",
    "            'imperial gallon': 'imperial gallon', 'l': 'litre', 'litre': 'litre', 'µl': 'microlitre', 'microlitre': 'microlitre',\n",
    "            'ml': 'millilitre', 'millilitre': 'millilitre', 'pt': 'pint', 'pint': 'pint', 'qt': 'quart', 'quart': 'quart'\n",
    "        }\n",
    "        self.unique_metrics = list(set(self.map_to_unit.values()))\n",
    "        self.__fit_one_hot()\n",
    "        \n",
    "# Example usage\n",
    "    def __get_full_unit_name(self,unit):\n",
    "        if unit in self.unique_metrics:\n",
    "            return unit\n",
    "        out = self.map_to_unit.get(unit.lower(), unit)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def get_one_hot_encode(self):\n",
    "        return self.s_one_hot_encode\n",
    "\n",
    "    def __fit_one_hot(self):\n",
    "        self.s_one_hot_encode.fit(self.data[['entity_name']])\n",
    "        self.met_one_hot.fit([self.unique_metrics])\n",
    "\n",
    "    def extract_value_unit_from_image(self,image_path):\n",
    "            img = Image.open(image_path)\n",
    "            gray_img = ImageOps.grayscale(img)\n",
    "            \n",
    "            np_image = np.array(gray_img)\n",
    "            result = self.ocr.ocr(np_image, cls=True)\n",
    "            extracted_text = \" \".join([res[1][0] for res in result[0]])\n",
    "            pattern = r'(\\d+\\.?\\d*)\\s?(cm³|liters|ml|g|kg|m³|in³|L|oz|fl oz|lb|centimetre|gram|l|pt|metre|g|cm|ton|ft|volt|millilitre|millimetre|kg|v|millivolt|imperial gallon|centilitre|cl|gal|m|kv|microlitre|qt|mv|microgram|w|milligram|µl|lbs|imp gal|fluid ounce|litre|tons|gallon|pound|quart|µg|foot|mg|ounce|mcg|kilovolt|cubic foot|gm|kilowatt|yd|cup|dl|oz|mm|cu ft|kw|cubic inch|gms|yard|kilogram|in|watt|fl oz|inch|decilitre|ml|pint|cu in)'  # Modify as per expected units\n",
    "            matches = re.findall(pattern, extracted_text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                return matches\n",
    "            else:\n",
    "                return \"\"\n",
    "\n",
    "    def __transform_for_one_record(self,record):\n",
    "        image_link ,group_id, entity_name, entity_value = None, None, None, None\n",
    "        if self.kind == \"train\":\n",
    "            image_link ,group_id, entity_name, entity_value = record['image_link'], record['group_id'], record['entity_name'], record['entity_value']\n",
    "        elif self.kind == \"test\":\n",
    "            image_link ,group_id, entity_name = record['image_link'], record['group_id'], record['entity_name']\n",
    "        img_path = os.path.join(self.image_root_path,image_link.split(\"/\")[-1])\n",
    "        transformed_img = self.ocr_feature_extrac.extract_features(img_path)\n",
    "        transformed_grp = self.group_id_mappings[group_id]\n",
    "        transformed_entity_name = self.one_hot_encode.transform([[entity_name]]).toarray()\n",
    "        if entity_value:\n",
    "            value, unit = entity_value.split(\" \")\n",
    "            value = float(value)\n",
    "            transformed_unit = self.met_one_hot.transform([unit])\n",
    "            entity_value = (value, transformed_unit)\n",
    "        \n",
    "        matches = self.extract_value_unit_from_image(img_path)\n",
    "        if matches: \n",
    "            metric_set = []\n",
    "            for i in range(len(matches)):\n",
    "                val , uni = matches[i][0], matches[i][1]\n",
    "\n",
    "                uni = uni.strip()\n",
    "                mapped_unit = self.__get_full_unit_name(uni)\n",
    "\n",
    "                transformed_unit = self.met_one_hot.transform([mapped_unit])\n",
    "\n",
    "                val = float(val)\n",
    "\n",
    "                metric_set.append([val,transformed_unit])\n",
    "            if len(metric_set) > self.max_metric_count:\n",
    "                metric_set = metric_set[:self.max_metric_count]\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if \n",
    "        return transformed_img, transformed_grp, transformed_entity_name, entity_value\n",
    "    \n",
    "    def preprocess():\n",
    "    \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2016dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/14 23:20:45] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\ASUS\\\\Desktop\\\\student_resource 3\\\\env\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\ASUS/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')  # Modify the parameters if necessary\n",
    "\n",
    "def extract_value_unit_from_image(image_path):\n",
    "    # Open and preprocess the image\n",
    "    img = Image.open(image_path)\n",
    "    gray_img = ImageOps.grayscale(img)\n",
    "    \n",
    "    # Convert the image to a numpy array for PaddleOCR\n",
    "    np_image = np.array(gray_img)\n",
    "    \n",
    "    # Perform OCR using PaddleOCR\n",
    "    result = ocr.ocr(np_image, cls=True)\n",
    "    \n",
    "    # Extract the recognized text\n",
    "    extracted_text = \" \".join([res[1][0] for res in result[0]])  # Concatenate the OCR text\n",
    "    \n",
    "    # Use regex to find value + unit (e.g., numbers followed by units like cm³, L, ml, etc.)\n",
    "    pattern = r'(\\d+\\.?\\d*)\\s?(cm³|liters|ml|g|kg|m³|in³|L|oz|fl oz|gallon|µg|kg|ml|litre|fluid ounce|m|ft|quart|kv|fl oz|volt|pt|yd|in|v|tons|gal|l|ounce|cl|µl|lbs|microlitre|mg|w|cubic inch|cm|lb|cubic foot|watt|cu in|cu ft|g|pint|mcg|ton|dl|decilitre|imp gal|cup|gms|gm|centilitre|kw|imperial gallon|mm|millilitre|oz|qt|mv|gallon)'  # Modify as per expected units\n",
    "    matches = re.findall(pattern, extracted_text, re.IGNORECASE)\n",
    "    \n",
    "    if matches:\n",
    "        # Return the first match (value + unit)\n",
    "        return matches\n",
    "    else:\n",
    "        # Return blank if no match is found\n",
    "        return \" \"\n",
    "\n",
    "# Example usage:\n",
    "# result = extract_value_unit_from_image('path_to_image.jpg')\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f547dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/14 23:20:55] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.08312630653381348\n",
      "[2024/09/14 23:20:55] ppocr DEBUG: cls num  : 4, elapsed : 0.026586055755615234\n",
      "[2024/09/14 23:20:55] ppocr DEBUG: rec_res num  : 4, elapsed : 0.11325645446777344\n"
     ]
    }
   ],
   "source": [
    "matches = extract_value_unit_from_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397c64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.985 cm\n",
      "2.75 in\n",
      "182.88 cm\n",
      "381 cm\n",
      "72 in\n",
      "150 in\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(matches)):\n",
    "    result = extract_value_unit_from_image(i)\n",
    "    print(matches[i][0], matches[i][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b6a12ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reader = easyocr.Reader(['en'], gpu=True)  # Set gpu=True for GPU usage\n",
    "\n",
    "# Function to preprocess and perform OCR on the image URL\n",
    "def extract_value_unit_from_image(image_url):\n",
    "    try:\n",
    "        # Send a request to get the image from the URL\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "        # Open the image and convert it to grayscale\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        gray_img = ImageOps.grayscale(img)\n",
    "        \n",
    "        # Convert Pillow image to numpy array for OCR\n",
    "        np_image = np.array(gray_img)\n",
    "        \n",
    "        # Perform OCR using EasyOCR\n",
    "        result = reader.readtext(np_image, detail=0)  # detail=0 returns only the text\n",
    "        extracted_text = \" \".join(result)  # Join the OCR result into a single string\n",
    "        \n",
    "        # Use regex to find the value + unit (e.g., numbers followed by units like cm³, L, ml, etc.)\n",
    "        pattern = r'(\\d+\\.?\\d*)\\s?(cm³|liters|ml|g|kg|m³|in³|L|oz|fl oz|gallon|µg|kg|ml|litre|fluid ounce|m|ft|quart|kv|fl oz|volt|pt|yd|in|v|tons|gal|l|ounce|cl|µl|lbs|microlitre|mg|w|cubic inch|cm|lb|cubic foot|watt|cu in|cu ft|g|pint|mcg|ton|dl|decilitre|imp gal|cup|gms|gm|centilitre|kw|imperial gallon|mm|millilitre|oz|qt|mv)'  # Modify as per expected units\n",
    "        matches = re.findall(pattern, extracted_text, re.IGNORECASE)\n",
    "        \n",
    "        if matches:\n",
    "            # Return the first match (value + unit)\n",
    "            return f\"{matches[0][0]} {matches[0][1]}\"\n",
    "        else:\n",
    "            # Return blank if no match is found\n",
    "            return \" \"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading image from {image_url}: {e}\")\n",
    "        return \" \"  # Return blank in case of an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f29b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_set = {'cm', 'ft', 'in', 'm', 'mm', 'yd','g', 'gm', 'gms', 'kg', 'µg', 'mcg', 'mg', 'oz', 'ounce', 'lb', 'lbs', 'ton', 'tons','kv', 'mv', 'v', 'volt', 'kw', 'w', 'watt','cl', 'centilitre', 'cu ft', 'cubic foot', 'cu in', 'cubic inch', 'cup', 'dl', 'decilitre', 'fl oz', 'fluid ounce', 'gal', 'gallon', 'imp gal', 'imperial gallon', 'l', 'litre', 'µl', 'microlitre', 'ml', 'millilitre', 'pt', 'pint', 'qt', 'quart', 'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard', 'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton', 'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton', 'kilovolt', 'millivolt', 'volt' , 'kilowatt', 'watt', 'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59ae73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43843b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = '|'.join(unit for unit in unit_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a0ce88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lb|centimetre|gram|l|pt|metre|g|cm|ton|ft|volt|millilitre|millimetre|kg|v|millivolt|imperial gallon|centilitre|cl|gal|m|kv|microlitre|qt|mv|microgram|w|milligram|µl|lbs|imp gal|fluid ounce|litre|tons|gallon|pound|quart|µg|foot|mg|ounce|mcg|kilovolt|cubic foot|gm|kilowatt|yd|cup|dl|oz|mm|cu ft|kw|cubic inch|gms|yard|kilogram|in|watt|fl oz|inch|decilitre|ml|pint|cu in'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33621d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['item_weight', 'item_volume', 'voltage', 'wattage',\n",
       "       'maximum_weight_recommendation', 'height', 'depth', 'width'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['entity_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22929cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_value_unit_from_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b794f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051818f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.fit(train[['entity_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12f07d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.transform([[\"item_volume\"]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "500e080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out = embed.get_embeddings(\"voltage\",\"DistilBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4866097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_link      https://m.media-amazon.com/images/I/61I9XdN6OF...\n",
       "group_id                                                   748919\n",
       "entity_name                                           item_weight\n",
       "entity_value                                           500.0 gram\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8654cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "facc28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\Desktop\\student_resource 3\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--naver-clova-ix--donut-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "mod = OCRFeatureExtractor(model_name=\"Donut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b1a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mod.extract_features(\"C://Users//ASUS//Desktop//student_resource 3//images//31+ZLMVIYaL.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a144599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 1024)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f45948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "028a83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pca.fit_transform(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f0c044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9deacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17dd8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nn.Linear(200, 200)\n",
    "n2 = nn.Linear(200,50)\n",
    "n3 = nn.Linear(240000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb2bfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = n2(n(torch.tensor(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5865bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.view(out.shape[0]*out.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06b0fa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "260f4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = n3(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f21094a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4fd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
